{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/Users/daniellee/Dropbox/Data_Mining/elevator_terrorist_detection/'\n",
    "df = pd.read_csv(path + 'simdata/simdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add event id \n",
    "df['event_id'] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21941, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Trip Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Time Variables\n",
    "def create_time_variables(df):\n",
    "    df = df.copy() \n",
    "    df['timein'], df['timeout'] = [pd.to_datetime(df[v]) for v in ['timein','timeout']]\n",
    "    df['womin'] = df.timein.dt.week # Week of Month\n",
    "    df['dowin'] = df.timein.dt.dayofweek # Day of Week\n",
    "    df['hourin'] = df.timein.dt.hour \n",
    "    df['minin'] = df.timein.dt.minute\n",
    "\n",
    "    df['womout'] = df.timeout.dt.week # Week of Month\n",
    "    df['dowout'] = df.timeout.dt.dayofweek # Day of Week\n",
    "    df['hourout'] = df.timeout.dt.hour \n",
    "    df['minout'] = df.timeout.dt.minute\n",
    "\n",
    "    # Binned Hours\n",
    "    def bin_hours(x):\n",
    "        if 5 <= x <= 10: return 0  # Morning commute\n",
    "        if 11 <= x <= 14: return 1 # Lunch \n",
    "        if 15 <= x <= 16: return 2 # Rush hour commute\n",
    "        if 17 <= x <= 23: return 3 # Evening \n",
    "        if 0 <= x <= 4: return 4  # Midnight\n",
    "\n",
    "    df['binned_hours'] = df.hourin.apply(lambda x: bin_hours(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of people encountered during the trip\n",
    "def number_of_people_encountered(df):\n",
    "    df = df.copy()\n",
    "    size = df.shape[0]\n",
    "    df_timein = df.timein\n",
    "    df_timeout = df.timeout\n",
    "    df_elevator = df.elevator\n",
    "    i = 0\n",
    "    for e, _in, _out in df[['elevator','timein','timeout']].values:\n",
    "        cond = (\n",
    "               (e == df_elevator) &\n",
    "               (((df_timein <= _in) & (_out <= df_timein)) | \n",
    "               ((_in <= df_timein) & (df_timein <= _out) & (_out <= df_timeout)) | \n",
    "               ((df_timein <= _in) & (_in <= df_timeout) & (df_timeout <= _out)) | \n",
    "               ((_in <= df_timein) & (df_timeout <= _out)))\n",
    "        )\n",
    "        temp = df.loc[cond]\n",
    "        df.loc[i, 'passenger_encountered'] = df.loc[cond, 'id'].unique().shape[0] - 1\n",
    "        if i % 1000 == 0: print(str((i + 1000)/ size * 100) + '% Complete...') \n",
    "        i += 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passenger Day Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_passengers_day_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Number of trips made on a given day \n",
    "    trips_per_day = df.groupby(['id','dowin','womin'])['elevator'].count()\n",
    "    \n",
    "    # Hours between first entered and exit\n",
    "    def work_hours(x):\n",
    "        min_time = x['timein'].min()\n",
    "        max_time = x['timeout'].max() \n",
    "        return (max_time - min_time).seconds / 3600\n",
    "\n",
    "    work_hours_df = df.groupby(['id','dowin','womin'])['timein','timeout'].apply(lambda x: work_hours(x))\n",
    "    \n",
    "    # Joined passenger day features\n",
    "    joined_passenger_day_feats = (\n",
    "                pd.concat([work_hours_df,trips_per_day],axis=1)\n",
    "                .rename(columns={0: 'work_hours', 'elevator': 'trip_cnts'})\n",
    "                .reset_index()\n",
    "    )\n",
    "    \n",
    "    return df.merge(joined_passenger_day_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traffic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_traffic_features(df):\n",
    "    df = df.copy() \n",
    "    \n",
    "    # Number of people who entered the building in a given day\n",
    "    people_count_series =  (\n",
    "                    df.groupby(['dowin','womin'])['timein'].count()\n",
    "                     .reset_index()\n",
    "                     .rename(columns={'timein': 'cnt_visitors'})\n",
    "    )\n",
    "    \n",
    "    # Number of people who entered the floor in a given day, binned hour comb\n",
    "    def get_unique_id(x):\n",
    "        return x.id.unique().shape[0]\n",
    "\n",
    "    people_count_dowin_combo = (\n",
    "                    df.groupby(['dowin','womin','binned_hours'])\n",
    "                    .apply(lambda x: get_unique_id(x))\n",
    "                    .reset_index()\n",
    "                    .rename(columns={0: 'cnt_of_binned_visitors'})\n",
    "    )\n",
    "    \n",
    "    df = df.merge(people_count_series).merge(people_count_dowin_combo)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lifetime Passenger Profile Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lifetime_profile_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Number of trips\n",
    "    number_of_trips = (\n",
    "                df.groupby('id')\n",
    "                .apply(lambda x: x.shape[0])\n",
    "                .to_frame()\n",
    "                .rename(columns={0: 'passenger_#_trips'})\n",
    "    ) \n",
    "    \n",
    "    # Probability distribution of trips made per day of week\n",
    "    dow_mass = pd.crosstab(df.id, df.dowin, normalize='index')\n",
    "    dow_mass.columns = [str(k)+ '_dow_mass' for k in dow_mass.columns]\n",
    "    \n",
    "    # Probability mass function of binned hours when trips are made\n",
    "    hour_mass = pd.crosstab(df.id, df.binned_hours, normalize='index')\n",
    "    hour_mass.columns = [str(k)+ 'hour_mass' for k in hour_mass.columns]\n",
    "\n",
    "    # Probability distribution of floor_in and floor_out\n",
    "    floorin_mass = pd.crosstab(df.id, df.floorin, normalize='index')\n",
    "    floorout_mass = pd.crosstab(df.id, df.floorout, normalize='index') \n",
    "    floorin_mass.columns = [str(k)+ 'floorin_mass' for k in floorin_mass.columns]\n",
    "    floorout_mass.columns = [str(k)+ 'floorout_mass' for k in floorout_mass.columns]\n",
    "    \n",
    "    joined_feats = pd.concat([number_of_trips, dow_mass, hour_mass, floorin_mass, floorout_mass],axis=1)\n",
    "    \n",
    "    return pd.merge(joined_feats, df, left_index = True, right_on='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Network Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_network(df):\n",
    "    df = df.copy() \n",
    "    \n",
    "    G = nx.Graph()\n",
    "\n",
    "    network_set = {}\n",
    "    df_timein = df.timein\n",
    "    df_timeout = df.timeout\n",
    "    df_elevator = df.elevator\n",
    "    for id, e, event_id, _in, _out in df[['id','elevator','event_id','timein','timeout']].values:\n",
    "        cond = (\n",
    "               (e == df_elevator) &\n",
    "               (((df_timein <= _in) & (_out <= df_timein)) | \n",
    "               ((_in <= df_timein) & (df_timein <= _out) & (_out <= df_timeout)) | \n",
    "               ((df_timein <= _in) & (_in <= df_timeout) & (df_timeout <= _out)) | \n",
    "               ((_in <= df_timein) & (df_timeout <= _out)))\n",
    "        )\n",
    "        unique_id = set(df.loc[(df.id != id) & cond, 'id'].unique())\n",
    "        if id not in network_set:\n",
    "            network_set[id] = unique_id\n",
    "        else:\n",
    "            network_set[id] = network_set[id].union(set(unique_id))\n",
    "\n",
    "    for n in df.id.unique():\n",
    "        G.add_node(n)\n",
    "            \n",
    "    for k, s in network_set.items():\n",
    "        for v in list(s):\n",
    "            G.add_edge(k, v)\n",
    "            \n",
    "    return G\n",
    "\n",
    "def create_network_features(df, G):\n",
    "    df = df.copy()\n",
    "    degree_df = pd.DataFrame(list(G.degree()), columns=['id','degrees'])\n",
    "    centrality_df = pd.DataFrame([(k, v) for k, v in nx.degree_centrality(G).items()], columns=['id','centrality'])\n",
    "\n",
    "    return df.merge(degree_df).merge(centrality_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavioral Change Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def last_k_features(x, k):\n",
    "    \n",
    "    x['offset'] = pd.DatetimeIndex(x['timeout']) - pd.DateOffset(k)\n",
    "    \n",
    "    for i in x.index:\n",
    "        offsetdate, curdate, eventid = x.loc[i,['offset','timeout','event_id']].values\n",
    "        \n",
    "        window_x = x.loc[(x.timein >= offsetdate) & (x.timeout <= curdate)]\n",
    "        window_binned_hours = window_x.binned_hours.value_counts().index[0]\n",
    "        window_floorin = window_x.floorin.value_counts().index[0]\n",
    "        window_floorout = window_x.floorout.value_counts().index[0]\n",
    "        \n",
    "        x.loc[x.event_id == eventid, 'window_binned_hours'] = window_binned_hours\n",
    "        x.loc[x.event_id == eventid, 'window_floorin'] = window_floorin\n",
    "        x.loc[x.event_id == eventid, 'window_floorout'] = window_floorout\n",
    "        \n",
    "    return x[['id','event_id','window_binned_hours','window_floorin','window_floorout']]\n",
    "\n",
    "def create_lag_features(df, k):\n",
    "    df = df.copy() \n",
    "    lagged_df = df.groupby('id').apply(lambda x: last_k_features(x, k))\n",
    "    \n",
    "    return df.merge(lagged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n",
      "4.557677407593091% Complete...\n",
      "9.115354815186182% Complete...\n",
      "13.673032222779272% Complete...\n",
      "18.230709630372363% Complete...\n",
      "22.788387037965453% Complete...\n",
      "27.346064445558543% Complete...\n",
      "31.903741853151637% Complete...\n",
      "36.46141926074473% Complete...\n",
      "41.019096668337816% Complete...\n",
      "45.576774075930906% Complete...\n",
      "50.134451483524% Complete...\n",
      "54.692128891117086% Complete...\n",
      "59.24980629871017% Complete...\n",
      "63.80748370630327% Complete...\n",
      "68.36516111389635% Complete...\n",
      "72.92283852148945% Complete...\n",
      "77.48051592908254% Complete...\n",
      "82.03819333667563% Complete...\n",
      "86.59587074426872% Complete...\n",
      "91.15354815186181% Complete...\n",
      "95.7112255594549% Complete...\n",
      "100.268902967048% Complete...\n",
      "complete\n",
      "complete\n",
      "complete\n",
      "complete\n",
      "complete\n"
     ]
    }
   ],
   "source": [
    "def main(df):\n",
    "    \n",
    "    df = create_time_variables(df)\n",
    "    print('complete')\n",
    "    df = number_of_people_encountered(df)\n",
    "    print('complete')\n",
    "    df = create_passengers_day_features(df)\n",
    "    print('complete')\n",
    "    df = create_traffic_features(df)\n",
    "    print('complete')\n",
    "    df = create_lifetime_profile_features(df)\n",
    "    print('complete')\n",
    "    G =  create_network(df)\n",
    "    df = create_network_features(df, G)\n",
    "    print('complete')\n",
    "    return df, G\n",
    "\n",
    "data, G = main(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv(path+'feature_data.csv', index=False)\n",
    "nx.write_gpickle(G,path+'network.gpickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
